{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7263bc0e",
   "metadata": {},
   "source": [
    "BASIC TERMINOLOGY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a4794",
   "metadata": {},
   "source": [
    "- test step - eg. turn on the light in a car\n",
    "- test assertion - assertion of a specific case, eg. check if the lights after working\n",
    "- unit test - a smaller test, one that checks that a single component operates in the right way\n",
    "- integration testing - testing multiple components together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8144f80",
   "metadata": {},
   "source": [
    "USEFUL LIBRARIES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0b2ab3",
   "metadata": {},
   "source": [
    "- unittest - part of Python standard library\n",
    "- pytest - generally all-purpose but especially for Functional and API testing, can be used for simple and complex code, API, databases and UIs\n",
    "- pydoc - documenting and testing your code at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80290a3b",
   "metadata": {},
   "source": [
    "UNIT TEST/TEST CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6753853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum([1, 2, 3]) == 6, \"Should be 6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b245001a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Should be 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m6\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be 6\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Should be 6"
     ]
    }
   ],
   "source": [
    "assert sum([1, 1, 1]) == 6, \"Should be 6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "150d95f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sum():\n",
    "    assert sum([1, 2, 3]) == 6, \"Should be 6\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33c0cf",
   "metadata": {},
   "source": [
    "ASSERTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38050f10",
   "metadata": {},
   "source": [
    "- assertion is a validation of the tested objects' output against a known response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e70cba",
   "metadata": {},
   "source": [
    "ASSERTION TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb21592",
   "metadata": {},
   "outputs": [],
   "source": [
    ".assertEqual(a, b) # a == b\n",
    ".assertTrue(x) # bool(x) is True\n",
    ".assertFalse(x) # bool(x) is False\n",
    ".assertIs(a, b) and .assertIsNot # a is b\n",
    ".assertIsNone(x) and .assertIsNotNone # x is None\n",
    ".assertIn(a, b) and .assertNotIn # a in b\n",
    ".assertIsInstance(a, b) and .assertIsNotInstance # isinstance(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272243a8",
   "metadata": {},
   "source": [
    "ENTRY POINT OF A TEST CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2761819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4babc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sum_tuple():\n",
    "    assert sum((1, 2, 2)) == 6, \"Should be 6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ec6d893",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Should be 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_sum_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mtest_sum_tuple\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_sum_tuple\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28msum\u001b[39m((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m6\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be 6\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Should be 6"
     ]
    }
   ],
   "source": [
    "test_sum_tuple()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e611842",
   "metadata": {},
   "source": [
    "TEST RUNNER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5936213",
   "metadata": {},
   "source": [
    "- special application designed for running tests, checking the output, and giving you tools for debugging and diagnosing tests and applications\n",
    "- unittest, nose and pytest are test runners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da65a73",
   "metadata": {},
   "source": [
    "UNITTEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb1dba6",
   "metadata": {},
   "source": [
    "- unit test requires that you put your tests into classes as methods\n",
    "- you use a series of special assertion methods in the unittest.TestCase class instead of the built-in assert statement\n",
    "- in Python 2 unittest is called unittest2, in Python 3 its called unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d73fb679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestSum(unittest.TestCase):\n",
    "    \n",
    "    def test_sum(self):\n",
    "        self.assertEqual(sum([1, 2, 3]), 6, \"Should be 6\")\n",
    "        \n",
    "    def test_sum_tuple(self):\n",
    "        self.assertEqual(sum([1, 2, 2]), 6, \"Should be 6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31b95198",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = TestSum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd521aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.test_sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfb90fd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "5 != 6 : Should be 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_sum_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36mTestSum.test_sum_tuple\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_sum_tuple\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massertEqual\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mShould be 6\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\unittest\\case.py:837\u001b[0m, in \u001b[0;36mTestCase.assertEqual\u001b[1;34m(self, first, second, msg)\u001b[0m\n\u001b[0;32m    833\u001b[0m \u001b[38;5;124;03m\"\"\"Fail if the two objects are unequal as determined by the '=='\u001b[39;00m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;124;03m   operator.\u001b[39;00m\n\u001b[0;32m    835\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    836\u001b[0m assertion_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getAssertEqualityFunc(first, second)\n\u001b[1;32m--> 837\u001b[0m \u001b[43massertion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\unittest\\case.py:830\u001b[0m, in \u001b[0;36mTestCase._baseAssertEqual\u001b[1;34m(self, first, second, msg)\u001b[0m\n\u001b[0;32m    828\u001b[0m standardMsg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m _common_shorten_repr(first, second)\n\u001b[0;32m    829\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_formatMessage(msg, standardMsg)\n\u001b[1;32m--> 830\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfailureException(msg)\n",
      "\u001b[1;31mAssertionError\u001b[0m: 5 != 6 : Should be 6"
     ]
    }
   ],
   "source": [
    "c.test_sum_tuple()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d314269",
   "metadata": {},
   "source": [
    "NOSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eb0483",
   "metadata": {},
   "source": [
    "- nose is compatible with any tests written using unittest framework and can be used as a drop-in replacement for the unittest test runner\n",
    "- the development of nose as an open-source applicaiton fell behind, and a fork called nose2 was created. If you're starting from scratch, it is recommended that you use nose2 instead of nose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22479621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nose2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f9deee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m nose2 # command line from the path where test cases live"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e7a1cf",
   "metadata": {},
   "source": [
    "PYTEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a926219",
   "metadata": {},
   "source": [
    "- pytest supports execution of unittest test cases\n",
    "- the real advantage of pytest comes by writing pytest test cases\n",
    "- pytest test cases are a series of functions in a Python file starting with the name test_\n",
    "- support for the built-in assert statement instead of using special self.assert*() methods\n",
    "- support for filtering for test cases\n",
    "- ability to re-run from the last failing test\n",
    "- an ecosystem of hundreds of plugins to extend the functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc5a1b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sum():\n",
    "    assert sum([1, 2, 3]) == 6, \"Should be 6\"\n",
    "\n",
    "def test_sum_tuple():\n",
    "    assert sum([1, 2, 2]) == 6, \"Should be 6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e89ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13ca681a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Should be 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_sum_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36mtest_sum_tuple\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_sum_tuple\u001b[39m():\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m6\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be 6\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Should be 6"
     ]
    }
   ],
   "source": [
    "test_sum_tuple()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f737feb0",
   "metadata": {},
   "source": [
    "SIDE EFFECTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9978924a",
   "metadata": {},
   "source": [
    "- when executing a piece of code, other things in the environment will be altered such as the attribute of a class, a file on the filesystem, value in a database\n",
    "- these are known as side-effects, and are an important part of testing\n",
    "- decide if the side effect is being tested before including it in your list of assertions\n",
    "- if you find that the unit of code to test has lots of side effects, you might be breaking the Single Responsibility Principle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17245b13",
   "metadata": {},
   "source": [
    "SINGLE RESPONSIBILITY PRINCIPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e1c4d0",
   "metadata": {},
   "source": [
    "- single reponsibility principle means the piece of code is doing too many things and would be better off being refactored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54fe68f",
   "metadata": {},
   "source": [
    "UNITTEST.MAIN() - HOW DOES UNITTEST KNOW WHICH CLASSES ARE TESTS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed06ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98933ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m unittest test # run this from concole\n",
    "python -m unittest -v test # more verbose version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923f2e4b",
   "metadata": {},
   "source": [
    "- this command executed test runner, discovering all classes in this file that inherit from unittest.TestCase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a6f9e6",
   "metadata": {},
   "source": [
    "DISCOVERY MODE OF UNITTEST RUNNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eba13c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m unittest discover\n",
    "# no need to specify the file name, all files which start with test*.py will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8227fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m unittest discover -s tests\n",
    "# pass the name of a folder which holds different test*py files, all will be run in one test plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b7f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m unittest discover -s tests -t src\n",
    "#if your folder with test*.py files is in a different folder, you can use -t and pass the path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e37ab3",
   "metadata": {},
   "source": [
    "UNDERSTANDING TEST OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9297a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.\n",
    "======================================================================\n",
    "FAIL: test_list_fraction (test.TestSum)\n",
    "----------------------------------------------------------------------\n",
    "Traceback (most recent call last):\n",
    "  File \"test.py\", line 21, in test_list_fraction\n",
    "    self.assertEqual(result, 1)\n",
    "AssertionError: Fraction(9, 10) != 1\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "Ran 2 tests in 0.001s\n",
    "\n",
    "FAILED (failures=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "F - means that one test failed\n",
    ". - means that one test passed\n",
    "FAIL: test_list_fraction - test method name\n",
    "test.TestSum - test module and test case\n",
    "Traceback to the failing line\n",
    "Fraction(9, 10) != 1 - details of the assertion with the expected (1) and actual (9,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd4d90c",
   "metadata": {},
   "source": [
    "TESTING IN WEB FRAMEWORKS LIKE DJANGO OR FLASK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f83419",
   "metadata": {},
   "source": [
    "- Django and Flask both provide a test framework based on unittest\n",
    "- you can continue writing tests in the unittest like fashion but execute them slightly different\n",
    "- the major difference is that you have to inherit from django.test.TestCase instead of unittest.TestCase\n",
    "- these classes have the same API, but the Django TestCase class sets up all the required state to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f54b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from django.test import TestCase\n",
    "\n",
    "class MyTestCase(TestCase):\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715be9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "python manage.py test # to execute your test suite from command line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daced5b3",
   "metadata": {},
   "source": [
    "- if you want multiple test files, replace tests.py with a folder called tests, insert an empty file called __init__.py, and create your test_*.py files. Django will discover and execute these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff188f17",
   "metadata": {},
   "source": [
    "FIXTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bdfe74",
   "metadata": {},
   "source": [
    "- fixture is data that you create as an input (data from a database, API, command-line, HTTP request, network socket, configuration file)\n",
    "- it's a common practice to create fixtures and reuse them\n",
    "- it's a good practice to store the test data in a folder within your integration testing folder called 'fixtures' to indicate that it contains test data, then from within your tests, you can load the data and run the tests\n",
    "- example fixtures: test_basic.json\n",
    "- if your application depends on data from a remote location, like a remote API, it's best practice to store remote fixtures locally so they can be recalled and sent to the application\n",
    "- requests library has a complimentary package called 'responses' that gives you ways to create response fixtures and save them in your test folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbc59a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestBasic(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        # Load test data\n",
    "        self.app = App(database='fixtures/test_basic.json')\n",
    "\n",
    "    def test_customer_count(self):\n",
    "        self.assertEqual(len(self.app.customers), 100)\n",
    "\n",
    "    def test_existence_of_customer(self):\n",
    "        customer = self.app.get_customer(id=10)\n",
    "        self.assertEqual(customer.name, \"Org XYZ\")\n",
    "        self.assertEqual(customer.address, '10 Red Road, Reading')\n",
    "\n",
    "class TestComplexData(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        # load complex data\n",
    "        self.app = App(database='fixtures/tset_complex.json')\n",
    "\n",
    "    def test_customer_count(self):\n",
    "        self.assertEqual(len(self.app.customers), 10000)\n",
    "\n",
    "    def text_existence_of_customer(self):\n",
    "        customer = self.app.get_customer(id=9999)\n",
    "        self.assertEqual(customer.name, u'バナナ')\n",
    "        self.assertEqual(customer.address, '10 Red Road, Akihabara, Tokyo')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae17c2c",
   "metadata": {},
   "source": [
    "PARAMETERIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f05346f",
   "metadata": {},
   "source": [
    "- it's a data-driven testing that allows you to execute the same test multiple times using different parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2afef1",
   "metadata": {},
   "source": [
    "HANDLING EXPECTED ERORS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7a220a",
   "metadata": {},
   "source": [
    "- there is a special way to handle expected errors. You can use .assertRaises() as a context-manager, then inside the with block execute the test steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    ".assertRaises()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d5d71",
   "metadata": {},
   "source": [
    "INTEGRATION TESTING/TEST SUITE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d122e00",
   "metadata": {},
   "source": [
    "- integration testing is testing of multiple components of the application to check that they work together\n",
    "- usually they will have more side-effects than unit tests\n",
    "- integration tests will require more fixtures to be in place\n",
    "- it's a good practice to separate unit tests from integration tests (eg. placing them in different folders)\n",
    "- usually integration tests run for much longer so they can be run once before pushing into production instead of on every commit, like in case of test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe3b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m unittest discover -s tests/integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8260ea",
   "metadata": {},
   "source": [
    "TESTING IN MULTIPLE ENVIRONMENTS - TOX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fece104",
   "metadata": {},
   "source": [
    "- tox is an application that automates testing in multiple environments (eg. one set of tests for Python 2 and another for Python 3)\n",
    "- tox builds virtual envs based on your setup.py (PyPI file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd4a54",
   "metadata": {},
   "source": [
    "TOX CONF FILE (after tox-quickstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835252fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "[tox]\n",
    "envlist = py27, py36\n",
    "\n",
    "[testenv]\n",
    "deps =\n",
    "\n",
    "commands =\n",
    "    python -m unittest discover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76a517",
   "metadata": {},
   "source": [
    "More about Tox:\n",
    "https://realpython.com/python-testing/\n",
    "https://tox.wiki/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d9ebc6",
   "metadata": {},
   "source": [
    "CI/CD - CONTINOUS INTEGRATION/CONTINOUS DEPLOYMENT - TESTING AUTOMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036cc898",
   "metadata": {},
   "source": [
    "- there are some tools for executing tests automatically when you make changes and commit them to a source control repository like Git\n",
    "- these tools can run your tests, compile, publish and even deploy to production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee5d5e",
   "metadata": {},
   "source": [
    "TRAVIS CI - CONTINOUS INTEGRATION TOOL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eacf639",
   "metadata": {},
   "source": [
    "- travis CI works nicely with Python\n",
    "- tests can be executed in the cloud\n",
    "- it usese Python requirements.txt file\n",
    "- Travis will run tests every time you committed and pushed to your remote repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906b6702",
   "metadata": {},
   "source": [
    "https://travis-ci.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150222e2",
   "metadata": {},
   "source": [
    "PYTEST LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00bdb466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bfe563",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytest test_sample.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b6efe",
   "metadata": {},
   "source": [
    "BASIC EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65cfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# content of test_sample.py\n",
    "def func(x):\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "def test_answer():\n",
    "    assert func(3) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4c7cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "$ pytest\n",
    "=========================== test session starts ============================\n",
    "platform linux -- Python 3.x.y, pytest-7.x.y, pluggy-1.x.y\n",
    "rootdir: /home/sweet/project\n",
    "collected 1 item\n",
    "\n",
    "test_sample.py F                                                     [100%]\n",
    "\n",
    "================================= FAILURES =================================\n",
    "_______________________________ test_answer ________________________________\n",
    "\n",
    "    def test_answer():\n",
    ">       assert func(3) == 5\n",
    "E       assert 4 == 5\n",
    "E        +  where 4 = func(3)\n",
    "\n",
    "test_sample.py:6: AssertionError\n",
    "========================= short test summary info ==========================\n",
    "FAILED test_sample.py::test_answer - assert 4 == 5\n",
    "============================ 1 failed in 0.12s ============================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2dc530",
   "metadata": {},
   "source": [
    "- The [100%] refers to the overall progress of running all test cases. After it finishes, pytest then shows a failure report because func(3) does not return 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188aa08d",
   "metadata": {},
   "source": [
    "RUNNING MULTIPLE TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af69cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "- pytest will run all files of the form test_*.py and *_test.py in the current directory and its subdirectories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
